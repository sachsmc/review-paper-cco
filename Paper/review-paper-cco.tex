\documentclass[11pt]{article}
\usepackage{hyperref}
\usepackage[margin=35mm]{geometry}



\title{\bigskip \bigskip Statistical Principles for Omics-based Clinical Trials}

%\author{true}

\author{\Large Michael C Sachs\vspace{0.05in} \\ \normalsize\emph{National Cancer Institute} \\ \footnotesize \url{maito:michael.sachs@nih.gov}\vspace*{0.2in}\\ }

%\author{Michael C Sachs (National Cancer Institute)}

\date{\footnotesize October 2014. Incomplete Draft. Please do not cite without permission.}
\linespread{2}

\begin{document}  
		




\maketitle


\begin{abstract}

\noindent High-throughput technologies enable the measurement of a large number of
molecular characteristics from a small tissue sample. High-dimensional
molecular information (referred to as omics data) offers the possibility
of predicting the future outcome of a patient (prognosis) and predicting
the likely response to a specific treatment (prediction). Embedded in
the vast amount of data is the hope that there exists some signal that
will enable practitioners to deliver therapy personalized to the
molecular profile of a tumor, thereby improving health outcomes. The
challenges are to determine that the omics assays are valid and
reproducible in a clinical setting, to develop a valid and optimal
omics-based test that algorithmically determines the optimal treatment
regime, to evaluate that test in a powerful and unbiased manner, and
finally to demonstrate clinical utility: that the test under study
improves clinical outcome as compared to not using the test. We review
the statistical considerations involved in each of these stages,
specifically dealing with the challenges of high-dimensional, omics
data.

\smallskip
\noindent \textbf{Keywords.} genomics; personalized medicine; predictive biomarker; statistics

\end{abstract}


\section{Introduction}\label{introduction}

Omics technologies that generate a large amount of molecular data about
a cancerous tumor have the potential to provide accurate predictions of
a patient's prognosis and predictions of their response to a specific
treatment regime. The idea of omics-based biomarkers is that distinct
tumor types can be identified using the multi-dimensional molecular data
leading to treatment decisions personalized to that tumor type. An
omics-based test can guide the decisions to treat or not to treat and
help identify the particular therapy most likely to work. The challenge
is to identify and demonstrate definitively that the use of an
omics-based test improves clinical outcomes in a patient population.

An omics-based test can be used to predict a patient's prognosis, which
is their expected clinical outcome. A test that provides accurate
predictions of prognosis, regardless of treatment, is referred to as a
prognostic biomarker. A predictive omics test is one that accurately
predicts disease outcomes with the application of specific
interventions. Predictive markers are therefore useful for the selection
among two or more treatment options. Statistically, a prognostic test is
strongly associated with clinical outcome and a predictive test modifies
the association between treatment and clinical outcome (interaction).
High dimensional omics data can be used to identify specific molecular
targets as potential mechanisms for drug development, however the use of
omics technologies for drug development is beyond the scope of this
review.

The path from development to definitively evaluating an omics-based test
for prognosis or prediction of treatment response is long and arduous.
Often, the end goal is to develop a test suitable for use in a clinical
trial for guiding treatment. The oncology literature is full of reports
that develop and/or evaluate omics-based tools for prognosis and
prediction. Developing a simple test based on high-dimensional omics
data can be complex and often uses novel statistical methods. Definitive
evaluation of a prognostic or predictive test is costly and rife with
methodological pitfalls. We aim to review such issues, giving you the
resources to ask the right questions when critically weighing the
evidence presented in a report of an omics-based study. Ultimately, as a
practicing oncologist the question is: ``Is this omics-based test
something I want to use to improve patient care?''.

The long road to implementing a test in a practice starts with
analytical validation, that is, demonstrating that the omics-based assay
accurately and reproducibly measures the molecular quantities. After the
assay performance is established comes the test development and
preliminary evaluation. This involves reducing the high-dimensional data
into a one-dimensional quantity that will be used to make a decision.
This one-dimensional quantity is often a risk score: an estimate of the
probability of a specific clinical outcome. It is necessary to establish
the clinical validity of this risk score, that is, demonstrate that the
risk score is independently associated with clinical outcome. Care must
be taken to completely separate the development of the risk score from
the evaluation, otherwise estimates can be optimistically biased.
Finally, the risk score must be translated into a binary decision, often
using a threshold. It remains to demonstrate that the use of the test to
make this decision improves patient outcomes.

The following sections specify questions you should ask while reading a
report of an omics-based clinical study. We review the importance of
such questions, and common pitfalls to watch for. If you are reporting
on an omics-based trial, answers to these questions should be made clear
to the reader. Formal efforts to guide reporting have been developed,
such as the REMARK checklist (1), the GRIPS statement (2), and a third
guideline article that lacks an acronym (3). Our review reflects these
efforts through the readers' lens.

\section{Terminology}\label{terminology}

An omics-based test, or simple an \textbf{omics test}, is a mapping from
the set of features on the omics assay to a single number. This number
can be a binary value, such as good or poor prognosis, or it can provide
a continuous scale, such as a risk score. It must be feasible to perform
the test on an individual patient basis, by measuring the omics assay on
the individual's tissue. The assay generates lots of measurements, which
we will refer to as \textbf{features}, and then fixed mathematical
calculations are done to transform the many features into the single
test value. Examples of such features are gene expression values,
protein expression measurements, or genomic mutations.

Investigators determine the way that the mathematical calculations are
done in the \textbf{development phase}. Often, there is a complete
sample which is randomly allocated into \textbf{development} and
\textbf{validation} samples. These are also sometimes referred to as
\textbf{training} and \textbf{test} sets of samples. A report may cover
only one of the two steps. At the end of the development phase, the
model for the mathematical calculations is fixed and locked down.

That model is evaluated definitively in the \textbf{validation} phase in
a completely independent sample. In order for the validation to be
unbiased and definitive, it is imperative that no information from the
validation sample leaks into the development phase. The validation
should mimic realistic clinical use as much as possible, and that means
that no further refinement to the test is allowed based on the observed
results.

\section{What is the intended clinical
use?}\label{what-is-the-intended-clinical-use}

As with all clinical studies, the end goal is to improve patient care.
Omics studies are no different, and a clear statement of the intended
clinical use of the omics-test should be prominent. Carefully describing
the context for the use of the assay determines the type of study needed
to develop and validate it. The intended use of the assay also provides
an overarching context in which to interpret the population under study,
the assay measurements, and the statistical methods.

Omics-based tests in oncology generally are used for one of two clinical
purposes: prognostic or predictive. A \textbf{prognostic} test is used
to predict the likely clinical outcome of a patient. What is the
clinical use of such a prediction? Often a prognosis is used to guide
management of the disease. Patients with a very good prognosis may opt
not to receive any treatment, while patients with a poor prognosis may
opt for more aggressive treatment. An omics-based prognostic test that
is currently used in practice is EndoPredict, which is used to predict
recurrence in ER-positive, HER2-negative breast cancer (4). For patients
with a low risk of recurrence, it has been demonstrated that the risks
of chemotherapy do not outweigh the benefits. Prognostic tests are
clinically useful for guiding general disease management.

\textbf{Predictive} tests are most useful for selecting patient
populations for treatment with specific targeted therapies. This
presumes the existence of a particular molecular targeted therapy. The
predictive test is used to identify patients who will benefit from the
targeted therapy. Predictive tests are generally based on only one or a
few molecular characteristics that the therapy may target. For example,
HER-2 is a gene that predicts a more aggressive form of breast cancer.
Trastuzumab is a drug that specifically targets HER-2 and has been shown
to be effective in HER-2 positive breast cancer (5). While targeted
therapies generally target only one molecular characteristic, omics
assays can be used to identify molecular targets for less
well-understood drugs. However, most successful targeted therapies have
associated predictive tests that were developed based on the underlying
biology rather than a broad search over a large number of molecular
features (6).

\section{What is the patient population of
interest?}\label{what-is-the-patient-population-of-interest}

Along with the intended clinical use, a report should have a clear
statement of the intended population in which the test is being
evaluated. This could be broad or quite specific. For the omics test to
be useful, it must provide sufficient information above and beyond the
standard of care in the target patient population. The distribution of
the omics test and the expected benefit in the population should be
clearly specified in advance.

The expected benefit of a new omics-based test could differ greatly by
patient population. For instance, a prognostic test has more potential
for benefit in stage 2 breast cancer than it does in stage 1 breast
cancer, as the prognosis for stage 1 is already very good. Evaluating an
omics-based test in a broad populations that encompasses multiple stages
or multiple disease types can be difficult, as the test must provide
more information beyond that provided by standard clinical and
pathological factors.

\section{Is the omics assay valid?}\label{is-the-omics-assay-valid}

Analytical validation of an assay involves evaluating the performance of
the measurement in terms of accuracy, bias, and precision under a
variety of conditions. Conditions are things like pre-analytic factors
such as specimen quality, specimen collection, storage, and processing
procedures, and technical aspects such as laboratory technician and
batch effects from reagent lots or other assay materials. The
high-dimensional nature of omics data makes it very difficult to assess
each of the hundreds or thousands of outputs from a single assay. In
developing a omics-based signature that only uses a subset of the
components of a high-dimensional assay, one can analytically validate
the final signature alone. However, prior to developing the signature,
one must develop detailed standard operating procedures for specimen
handling and processing to ensure a baseline level of validity.

Did the authors of the report state what type of specimens were used in
the study? Can the test be applied to formalin-fixed paraffin embedded
(FFPE) tissue, or only fresh-frozen? Most omics-based assays require a
minimum percentage of tumor to be successful. A report should clearly
state what criteria were used to screen tissue samples prior to running
the assay. Generally this involves a criteria for the rejection of
poor-quality specimens on the basis of percent tumor, percent necrosis,
or some other marker of tissue quality.

Molecular assays can successfully be run on decades old FFPE tissue (7).
However, factors involved in the tissue processing and storage can
impact the results (8--10). Due to the high dimensionality of omics
assays, a small amount of bias on each feature can translate into large
errors when incorporating data from hundreds or thousands of features
into a single continuous measurement. Therefore it is important to
assess the impact of processing on the individual features in addition
to the overall test.

In addition to processing and storage, technical aspects of an assay can
impact the final results in a predictable way (11,12). There could be
technical effects, differences due to reagent lots, and other batch
effects. Such batch effects are commonly recognized yet often ignored in
high-dimensional assays (13). Efforts should be made to measure the
impact of these technical aspects and minimize them to the greatest
extent possible. The way in which samples are assayed should be
randomized to prevent confounding batch effects with the clinical
outcome. Development and validation samples are sometimes run in the
same batch or with the same lot of technical aspects. This does minimize
batch effects, however, it can provide an overly optimistic assessment
of the test, because in clinical use, running things in the same batch
is not an option.

\section{On what samples was the test
developed?}\label{on-what-samples-was-the-test-developed}

Similar to developing criteria for rejection of tissue samples, in omics
settings, criteria should be developed for the rejection of individual
features (e.g.~genes, proteins) prior to the development of the test.
Features that do not pass the pre-specified quality metrics should be
removed from consideration from the final test. Note that this feature
processing step does not involve any clinical outcome measurements. As a
concrete example, in the development of a gene expression based test,
investigators may choose to exclude probe locations that have a dynamic
range under some threshold, or probes for which only a small proportion
of the samples had calls, or probes that have absolute expression levels
below some threshold. Quality control steps like this can ensure a more
robust a reproducible development of the test.

Don't: confound technical factors with clinical outcomes. (13,14)

Do: maintain strict separation between development and evaluation.

Do: cross validation if you have a data-sparse setting. (15--18)

Don't: use convoluted methods leading to overfitting.

\section{What does the omics-test
do?}\label{what-does-the-omics-test-do}

Once the analytical validity of the omics assay is established, the
features are translated into a binary classification, a multi-category
classification, or a continuous risk score. Carefully evaluate the
methods used to perform this translation and ask how are the features of
the omics assay translated into a clinically meaningful quantity?

Unfortunately, a common approach to developing prediction models is to
use cluster analysis of omics features, ignoring the clinical outcome
among the development samples. Cluster analysis is a class of methods
that is used to partition samples into groups based on the similarities
or differences among the omics features (19). The meaning and number
groups are not known in advance, but rather they are data dependent.
Clustering is unsupervised in the sense that the groups discovery is
done ignoring the true groups defined by the clinical outcome. The
resulting clusters are not designed to provide valid information
regarding a prognosis or prediction of response to therapy (20). A
common argument in favor of clustering is that it identifies
biologically distinct groups. However, the groups are identified using a
statistical algorithm and the biological relevance is only considered
\emph{post hoc}. For developing omics-based prognostic or predictive
tests, it is better to use statistical methods which are designed to
address those aims.

Often, there are more features measured than there are patients in the
sample. In such high-dimensional settings, it is required to identify a
subset of the features that will be used in the final multivariate
mathematical model. There are two broad statistical approaches to this
problem: \textbf{filtering} and \textbf{regularization}.

Filtering is a statistical approach where univariate methods are applied
to each of the many omics features in turn. Typically, the univariate
method involves estimating the association of the feature with the
clinical outcome. Then, some criterion, which is chosen in advance or
selected using cross-validation, is applied to the statistic to select a
subset of features. For example, I am interested in developing a gene
expression based test to predict clinical response to a new therapy. For
each of the 1000 gene expression features that I have, I can compute a
t-statistic comparing the expression levels for responders versus
non-responders. I then filter out the genes with t-test p-values greater
than 0.0001, and use the remaining ones in a multivariable logistic
regression model to predict response. (21) describes a novel approach to
filtering that is applied successfully to predict B-cell lymphoma
subtypes using gene expression microarrays.

Regularization is an approach where all of the features in consideration
are entered into a special multivariable statistical model for
prediction of the clinical outcome, even if there are more features than
samples. The special model includes a penalty component which encourages
the model to throw out or downplay the impact of features that are not
relevant. There are various types of penalty functions each with
different properties, such as the LASSO (22), the ridge penalty (23),
the elastic net (24), and others (19). Each type of penalty term
contains at least one tuning parameter, which may be pre-specified or
selected using cross-validation.

Each type of approach has its merits, and within each class there are a
variety of specific models to choose from. In real applications, it is
hard to determine what method will work best in advance. Instead of
selecting a single model to use, multiple models can be averaged to
improve prediction (25). This approach, called Bayesian model averaging
has proven successful in different applications, including prediction of
cancer subtypes (26). It is more common, however, to try out various
different methods then select the one with the best performance. This is
fine as long as the model selection is done entirely separated from the
final validation sample. Leaking of information from the validation data
into the model selection process can cause bias in insidious ways.
Verify that the model selection and estimation process was done
completely independently and locked down.

\section{On what samples is the test being
evaluated?}\label{on-what-samples-is-the-test-being-evaluated}

Do: define the clinical use (27)

Study design: consider retrospective (28)

Do: Design your study appropriately to answer the clinical question
definitively (29--40)

Do: Power your trial appropriately (41,42)

Don't: do partial resubstitution

\section{Are valid methods being used to evaluate the
test?}\label{are-valid-methods-being-used-to-evaluate-the-test}

Bad: IDI or net reclassification (43,44)

Bad: Comparing AUCs for regression models (45)

Good: comprehensive and pre-specified approach (46)

\section{Are the development and evaluation samples strictly
separated?}\label{are-the-development-and-evaluation-samples-strictly-separated}

This issue has come up in previous sections, yet this error occurs so
frequently that it needs to be highlighted in its own section. The
evaluation sample for the assessment of a prognostic or predictive test
needs to be completely independent from the development sample. This is
especially true for omics-based tests, whose development is often
complex and convoluted. Any information from the evaluation sample that
leaks into the development sample can bias the results, making tests
appear better than they truly are.

Leaking information between samples can happen in subtle ways.
Sometimes, part of the model development process is done on the
validation data again. This is called partial resubstitution (20). For
example, a common model development approach is to first filter a subset
of 50 genes from a larger set of 450,000 based on their observed
association with the outcome. Then, the 50 genes are put into a
regression model to develop a single risk score. Occasionally,
investigators will perform the filtering on the development sample and
then re-estimate the regression model using the combined development and
validation samples. This gives overly optimistic estimates of the
performance of the algorithm. Partial resubstitution can be difficult to
detect when the model development is more complex, and if
cross-validation is used to estimate the performance.

In settings where relatively few samples are available, cross-validation
is an efficient and valid approach to estimating performance (47). The
key point whether using the split sample approach or cross validation is
that the entire model building process must be validated. Even informal
checks of the model on the validation sample, such as viewing survival
curve plots, prior to locking down the model can unknowingly cause bias.
Therefore, once again we highlight the imperative that the validation
sample be strictly separated from the \textbf{entire} model development
process.

\section{Concluding remarks}\label{concluding-remarks}

Do: follow reporting criteria (1--3,48)

\section{References}\label{references}

\setlength{\parindent}{0pt}

1. Altman DG, McShane LM, Sauerbrei W, Taube SE. Reporting
recommendations for tumor marker prognostic studies (rEMARK):
Explanation and elaboration. BMC medicine. BioMed Central Ltd;
2012;10(1):51.

2. Janssens AC, Ioannidis J, Bedrosian S, Boffetta P, Dolan SM, Dowling
N, et al. Strengthening the reporting of genetic risk prediction studies
(gRIPS): Explanation and elaboration. European journal of clinical
investigation. Wiley Online Library; 2011;41(9):1010--35.

3. McShane LM, Cavenagh MM, Lively TG, Eberhard DA, Bigbee WL, Williams
PM, et al. Criteria for the use of omics-based predictors in clinical
trials: Explanation and elaboration. BMC medicine. BioMed Central Ltd;
2013;11(1):220.

4. Filipits M, Rudas M, Jakesz R, Dubsky P, Fitzal F, Singer CF, et al.
A new molecular predictor of distant recurrence in eR-positive,
hER2-negative breast cancer adds independent information to conventional
clinical risk factors. Clinical Cancer Research. American Association
for Cancer Research; 2011;17(18):6012--20.

5. Fleeman N, Bagust A, Beale S, Dwan K, Dickson R, Director L, et al.
Pertuzumab in combination with trastuzumab and docetaxel for the
treatment of hER2 positive metastatic or locally recurrent unresectable
breast cancer: A single technology appraisal. 2013;

6. Sawyers CL. The cancer biomarker problem. Nature. Nature Publishing
Group; 2008;452(7187):548--52.

7. Iwamoto KS, Mizuno T, Ito T, Akiyama M, Takeichi N, Mabuchi K, et al.
Feasibility of using decades-old archival tissues in molecular
oncology/epidemiology. The American journal of pathology. American
Society for Investigative Pathology; 1996;149(2):399.

8. Srinivasan M, Sedmak D, Jewell S. Effect of fixatives and tissue
processing on the content and integrity of nucleic acids. The American
journal of pathology. Elsevier; 2002;161(6):1961--71.

9. Maldegem F van, Wit M de, Morsink F, Musler A, Weegenaar J, Noesel CJ
van. Effects of processing delay, formalin fixation, and
immunohistochemistry on rNA recovery from formalin-fixed
paraffin-embedded tissue sections. Diagnostic Molecular Pathology. LWW;
2008;17(1):51--8.

10. Specht K, Richter T, M{ü}ller U, Walch A, Werner M, H{ö}fler H.
Quantitative gene expression analysis in microdissected archival
formalin-fixed and paraffin-embedded tumor tissue. The American journal
of pathology. Elsevier; 2001;158(2):419--29.

11. Pennello GA. Analytical and clinical evaluation of biomarkers
assays: When are biomarkers ready for prime time? Clinical Trials. SAGE
Publications; 2013;1740774513497541.

12. Isler JA, Vesterqvist OE, Burczynski ME. Analytical validation of
genotyping assays in the biomarker laboratory. Future Medicine Ltd;
2007;

13. Leek JT, Scharpf RB, Bravo HC, Simcha D, Langmead B, Johnson WE, et
al. Tackling the widespread and critical impact of batch effects in
high-throughput data. Nature Reviews Genetics. Nature Publishing Group;
2010;11(10):733--9.

14. Soneson C, Gerster S, Delorenzi M. Batch effect confounding leads to
strong bias in performance estimates obtained by cross-validation. PloS
one. Public Library of Science; 2014;9(6):e100335.

15. McShane LM, Polley M-YC. Development of omics-based clinical tests
for prognosis and therapy selection: The challenge of achieving
statistical robustness and clinical utility. Clinical Trials. SAGE
Publications; 2013;10(5):653--65.

16. Moons KG, Kengne AP, Woodward M, Royston P, Vergouwe Y, Altman DG,
et al. Risk prediction models: I. development, internal validation, and
assessing the incremental value of a new (bio) marker. Heart. BMJ
Publishing Group Ltd; British Cardiovascular Society;
2012;98(9):683--90.

17. Moons KG, Kengne AP, Grobbee DE, Royston P, Vergouwe Y, Altman DG,
et al. Risk prediction models: II. external validation, model updating,
and impact assessment. Heart. BMJ Publishing Group Ltd; British
Cardiovascular Society; 2012;heartjnl--l2011.

18. Polley M-YC, Freidlin B, Korn EL, Conley BA, Abrams JS, McShane LM.
Statistical and practical considerations for clinical evaluation of
predictive biomarkers. Journal of the National Cancer Institute. Oxford
University Press; 2013;105(22):1677--83.

19. Hastie T, Tibshirani R, Friedman J, Hastie T, Friedman J, Tibshirani
R. The elements of statistical learning. Springer; 2009.

20. Simon R, Radmacher MD, Dobbin K, McShane LM. Pitfalls in the use of
dNA microarray data for diagnostic and prognostic classification.
Journal of the National Cancer Institute. Oxford University Press;
2003;95(1):14--8.

21. Bair E, Tibshirani R. Semi-supervised methods to predict patient
survival from gene expression data. PLoS biology. Public Library of
Science; 2004;2(4):e108.

22. Tibshirani R. Regression shrinkage and selection via the lasso.
Journal of the Royal Statistical Society Series B (Methodological).
Blackwell Publishers; 1996;267--88.

23. Hoerl AE, Kennard RW. Ridge regression: Biased estimation for
nonorthogonal problems. Technometrics. Taylor \& Francis Group;
1970;12(1):55--67.

24. Zou H, Hastie T. Regularization and variable selection via the
elastic net. Journal of the Royal Statistical Society: Series B
(Statistical Methodology). Blackwell Publishing Ltd; 2005;67(2):301--20.

25. Hoeting JA, Madigan D, Raftery AE, Volinsky CT. Bayesian model
averaging: A tutorial. Statistical science. Institute of Mathematical
Statistics; 1999;382--401.

26. Yeung KY, Bumgarner RE, Raftery AE. Bayesian model averaging:
Development of an improved multi-class, gene selection and
classification tool for microarray data. Bioinformatics. Oxford
University Press; 2005;21(10):2394--402.

27. Mandrekar SJ, Sargent DJ. Predictive biomarker validation in
practice: Lessons from real trials. Clinical trials. SAGE Publications;
2010;7(5):567--73.

28. Simon RM, Paik S, Hayes DF. Use of archived specimens in evaluation
of prognostic and predictive biomarkers. Journal of the National Cancer
Institute. Oxford University Press; 2009;101(21):1446--52.

29. Freidlin B, Korn EL. Biomarker enrichment strategies: Matching trial
design to biomarker credentials. Nature Reviews Clinical Oncology.
Nature Publishing Group; 2014;11(2):81--90.

30. Baker SG, Sargent DJ. Designing a randomized clinical trial to
evaluate personalized medicine: A new approach based on risk prediction.
Journal of the National Cancer Institute. Oxford University Press; 2010;

31. Baker SG, Kramer BS, Sargent DJ, Bonetti M. Biomarkers, subgroup
evaluation, and clinical trial design. Discovery medicine.
2012;13(70):187--92.

32. Brannath W, Zuber E, Branson M, Bretz F, Gallo P, Posch M, et al.
Confirmatory adaptive designs with bayesian decision tools for a
targeted therapy in oncology. Statistics in medicine. Wiley Online
Library; 2009;28(10):1445--63.

33. Denne JS, Pennello G, Zhao L, Chang S-C, Althouse S. Identifying a
subpopulation for a tailored therapy: Bridging clinical efficacy from a
laboratory-developed assay to a validated in vitro diagnostic test kit.
Statistics in Biopharmaceutical Research. Taylor \& Francis;
2014;6(1):78--88.

34. Eng KH. Randomized reverse marker strategy design for prospective
biomarker validation. Statistics in medicine. Wiley Online Library;
2014;

35. Freidlin B, Jiang W, Simon R. The cross-validated adaptive signature
design. Clinical Cancer Research. AACR; 2010;16(2):691--8.

36. Freidlin B, McShane LM, Polley M-YC, Korn EL. Randomized phase iI
trial designs with biomarkers. Journal of Clinical Oncology. American
Society of Clinical Oncology; 2012;30(26):3304--9.

37. Freidlin B, Korn EL, Gray R. Marker sequential test (maST) design.
Clinical Trials. SAGE Publications; 2013;1740774513503739.

38. Jiang W, Freidlin B, Simon R. Biomarker-adaptive threshold design: A
procedure for evaluating treatment with possible biomarker-defined
subset effect. Journal of the National Cancer Institute. Oxford
University Press; 2007;99(13):1036--43.

39. Mandrekar SJ, Sargent DJ. Clinical trial designs for predictive
biomarker validation: Theoretical considerations and practical
challenges. Journal of Clinical Oncology. American Society of Clinical
Oncology; 2009;27(24):4027--34.

40. Morita S, Yamamoto H, Sugitani Y. Biomarker-based bayesian
randomized phase iI clinical trial design to identify a sensitive
patient subpopulation. Statistics in medicine. John Wiley \& Sons, Ltd;
2014;

41. Mackey HM, Bengtsson T. Sample size and threshold estimation for
clinical trials with predictive biomarkers. Contemporary clinical
trials. Elsevier; 2013;36(2):664--72.

42. Peterson B, George SL. Sample size requirements and length of study
for testing interaction in a 1\(\times\)\textless{} i\textgreater{}
k\textless{}/i\textgreater{} factorial design when time-to-failure is
the outcome. Controlled clinical trials. Elsevier; 1993;14(6):511--22.

43. Hilden J, Gerds TA. A note on the evaluation of novel biomarkers: Do
not rely on integrated discrimination improvement and net
reclassification index. Statistics in medicine. John Wiley \& Sons, Ltd;
2013;

44. Pepe MS. Problems with risk reclassification methods for evaluating
prediction models. American journal of epidemiology. Oxford University
Press; 2011;kwr013.

45. Seshan VE, G{ö}nen M, Begg CB. Comparing rOC curves derived from
regression models. Statistics in medicine. Wiley Online Library;
2013;32(9):1483--93.

46. Janes H, Brown MD, Huang Y, Pepe MS. An approach to evaluating and
comparing biomarkers for patient treatment selection. The international
journal of biostatistics. 2014;10(1):99--121.

47. Lee S. Mistakes in validating the accuracy of a prediction
classifier in high-dimensional butsmall-sample microarray data.
Statistical methods in medical research. SAGE Publications; 2008;

48. Bouwmeester W, Zuithoff NP, Mallett S, Geerlings MI, Vergouwe Y,
Steyerberg EW, et al. Reporting and methods in clinical prediction
research: A systematic review. PLoS medicine. Public Library of Science;
2012;9(5):e1001221.

\end{document}